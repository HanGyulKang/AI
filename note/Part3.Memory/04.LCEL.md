# LCEL 체인을 활용한 메모리

---


### Library
```python
from operator import itemgetter
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_ollama import ChatOllama
```

### LLM, Prompt 구성
```python
# AI 모델 초기화
model = ChatOllama(model="llava:latest", temperature=0)

# Prompt 설정
prompt = ChatPromptTemplate.from_messages(
    [
        # System 프롬프트
        ("system", "너는 수학 선생님이야."),
        # MessagePlaceholder로 공간을 잡아 줌.
        # 여기에 만든 variable_name이 대화 내역(history)의 임의 key 값
        MessagesPlaceholder(variable_name="{some_key}"),
        ("human", "{input}"),
    ]
)
```

### Memory 생성
```python
# 버퍼 메모리 생성, 여기에서 memory_key는 사전에 프롬프트에 설정했던 메모리 key 값
memory = ConversationBufferMemory(return_message=True, memory_key="{some_key}")
```

### Memory 출력
```python
print(memory.load_memory_variables({}))

# 저장 기록 출력 시 사전에 선언한 key값에 List가 담겨있는 것을 확인하 할 수 있다.
# [결과] :
# {'{sume_key}: []'}
```

### 사용
##### 주의사항
map 객체(현재 예제에서는 `{'{sume_key}: []'}`) 그대로 전달을 하게 되면 **오류가 발생하기 때문에**  
`ìtemgetter`를 사용하서 객체 안에 담겨있는 `List`를 추출한다.

```python
runnable = RunnablePassthrough.assing(
    # lambda로 memory에 저장된 대화 내역을 불러오는 함수를 그대로 넘긴다.
    chat_history=RunnableLambda(memory.load_memory_variables)
    | itemgetter("{some_key}")
)
```

### Chain 구성
```python
chain = runnable | prompt | model | StrOutputParser()
```

### 대화 내용 저장
* 대화 내용 저장은 질의시마다 해야 함.
```python
# 질의
response = chain.invoke({
    "input": {input}
})

# 질의와 응답 저장
memory.save_context(
    {
        "human": {input},
        "ai": response.content
    }
)
```