# LLM
이란? Large Language Model. 우리가 쓰고 있는 AI 모델들

---

#### OpenAI GPT
```python
modelName = "gpt-4o", "gpt-4.1-nano" 등등..
llm = ChatOpenAI(model_name="{modelName}")
```
#### Anthropic `Claude`
```python
from langchain_anthropic import ChatAnthropic

modelName = "claude-3-sonnet" 등등..
llm = ChatAnthropic(model="{modelName}")
```
#### 로컬 모델(`Ollama`)
```python
from langchain_community.chat_models import ChatOllama

modelName = "llama3:8b" 등등..
llm = ChatOllama(model="{modelName}")
```

기타 등등

---

## [Init]
```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain.globals import set_llm_cache


llm = ChatOpenAI(model_name="gpt-4.1-nano")
prompt = PromptTemplate.from_template("{num1} + {num2}의 결과를 알려줘")
chain = prompt | llm
```

## Caching
### InMemoryCache
```python
from langchain.cache import InMemoryCache


set_llm_cache(InMemoryCache()) # 메모리 캐싱
```

### Redis Cache
```python
from langchain.cache import RedisCache
import redis

redis_client = redis.Redis(host='localhost', port=6379) # redis
set_llm_cache(RedisCache(redis_client)) # redis에 캐싱
```


### SQLite Cache
```python
from langchain_community.cache import SQLiteCache
import os

# 캐시 디렉토리를 생성
if not os.path.exists("cache"):
    os.makedirs("cache")

set_llm_cache(SQLiteCache(database_path="cache/llm_cache.db")) # SQLiteCache
```

### > Result
```python
response = chain.invoke({"num1":3, "num2":12})
print(response)
```